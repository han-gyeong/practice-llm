{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# ollama pull deepseek-r1:1.5b\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#llm = ChatOllama(model=\"deepseek-r1:1.5b\")\u001B[39;00m\n\u001B[1;32m      5\u001B[0m llm \u001B[38;5;241m=\u001B[39m ChatOllama(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama3.2:1b\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/practice-llm/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:396\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    391\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    392\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    394\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    395\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m--> 396\u001B[0m             [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m],\n\u001B[1;32m    397\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    398\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    399\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    400\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    401\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    402\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    403\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    404\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    405\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/Workspace/practice-llm/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:381\u001B[0m, in \u001B[0;36mBaseChatModel._convert_input\u001B[0;34m(self, model_input)\u001B[0m\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatPromptValue(messages\u001B[38;5;241m=\u001B[39mconvert_to_messages(model_input))\n\u001B[1;32m    377\u001B[0m msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid input type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model_input)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    380\u001B[0m )\n\u001B[0;32m--> 381\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[0;31mValueError\u001B[0m: Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# ollama pull deepseek-r1:1.5b\n",
    "#llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "llm.invoke(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T15:35:13.619656Z",
     "start_time": "2025-12-24T15:35:13.580015Z"
    }
   },
   "id": "3d0c04ed02fc4340"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\" : \"France\"})\n",
    "print(prompt)\n",
    "\n",
    "llm.invoke(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-12-24T15:35:13.617095Z"
    }
   },
   "id": "2fd7fb58aed32b5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- System Message : LLM App의 목적, 페르소나\n",
    "- Human Message : 사람이 준 메시지\n",
    "- AI Message : LLM 에서 준 메시지\n",
    "- Tool Message : 도구의 실행 결과\n",
    "- message 들은 프롬프트 -> 인간의 질문 -> AI 답변 이런식으로 전체의 히스토리를 의미함,\n",
    "- AIMessage 가 맨 마지막일 경우 AI 는 이미 답변했다고 생각해서 답변을 안줌\n",
    "- One Shot / Few Shot 이라는 개념도 있는데, 이건 질문과 답변에 대한 예시를 전달해서 원하는 답변 유도 -> 정확도 향상\n",
    "\n",
    "```python\n",
    "# 이런걸 주는게 One shot(1개), Few shot(여러개)\n",
    "message = [\n",
    "    HumanMessage(content = \"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris\"),\n",
    "    HumanMessage(content = \"What is the capital of Italy?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Rome\"),\n",
    "]\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8df0f8dc070d7a6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Paris has been the capital of France since 987, when King Philip II Augustus captured the city from the Holy Roman Emperor Charles IV. At that time, the French Monarchy was based in Paris, and it remained the country's capital until the French Revolution in 1789.\\n\\nPrior to 987, other cities in France, such as Chartres and Reims, were also important centers of power and culture. However, after the Norman Conquest of England in 1066, English-speaking Normans began to settle in various parts of France, which led to a shift of power from the centralized French monarchy to regional dukes.\\n\\nIn 987, King Philip II Augustus captured Paris from the English and incorporated it into his kingdom. He declared that Paris would be the new capital of France, and since then, it has remained so.\\n\\nOver time, Paris has evolved into a global center of art, fashion, cuisine, and culture, and its importance as the capital of France has been solidified through various historical events, such as the French Revolution and World War II. Today, Paris is one of the most visited cities in the world, known for its iconic landmarks like the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-24T15:35:20.625264Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5131549083, 'load_duration': 89905333, 'prompt_eval_count': 61, 'prompt_eval_duration': 264804791, 'eval_count': 255, 'eval_duration': 2889025965, 'logprobs': None, 'model_name': 'llama3.2:1b'}, id='run--8d093ca5-7106-4ae2-bdca-60a4c119cfd2-0', usage_metadata={'input_tokens': 61, 'output_tokens': 255, 'total_tokens': 316})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "# System Message : LLM App의 목적, 페르소나\n",
    "# Human Message : 사람이 준 메시지\n",
    "# AI Message : LLM 에서 준 메시지\n",
    "# Tool Message : 도구의 실행 결과\n",
    "# message 들은 프롬프트 -> 인간의 질문 -> AI 답변 이런식으로 전체의 히스토리를 의미함,\n",
    "# AIMessage 가 맨 마지막일 경우 AI 는 이미 답변했다고 생각해서 답변을 안줌\n",
    "# One Shot / Few Shot 이라는 개념도 있는데, 이건 질문과 답변에 대한 예시를 전달해서 원하는 답변 유도 -> 정확도 향상\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content = \"You are a helpful assistant!\"),\n",
    "    HumanMessage(content = \"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris\"),\n",
    "    HumanMessage(content=\"Why Paris is capital of France?\")\n",
    "]\n",
    "\n",
    "llm.invoke(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T15:35:20.638840Z",
     "start_time": "2025-12-24T15:35:15.495161Z"
    }
   },
   "id": "af93fcd325d4dbab"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-24T15:35:20.946037Z', 'done': True, 'done_reason': 'stop', 'total_duration': 246761125, 'load_duration': 87288292, 'prompt_eval_count': 38, 'prompt_eval_duration': 21486458, 'eval_count': 8, 'eval_duration': 78094251, 'logprobs': None, 'model_name': 'llama3.2:1b'}, id='run--abb08ebc-98bd-4365-9880-6e1f35164083-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "message = [\n",
    "    (\"system\", \"You are a helpful assistant!\"),\n",
    "    (\"human\", \"What is the capital of {country}?\")\n",
    "]\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(message)\n",
    "chat_prompt = chat_prompt_template.invoke({\"country\" : \"France\"})\n",
    "\n",
    "print(chat_prompt)\n",
    "\n",
    "llm.invoke(chat_prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T15:35:20.952726Z",
     "start_time": "2025-12-24T15:35:20.640248Z"
    }
   },
   "id": "90fbd544746c4ce3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-24T15:35:21.192801Z', 'done': True, 'done_reason': 'stop', 'total_duration': 237850000, 'load_duration': 79387417, 'prompt_eval_count': 38, 'prompt_eval_duration': 22776292, 'eval_count': 8, 'eval_duration': 76726418, 'logprobs': None, 'model_name': 'llama3.2:1b'}, id='run--f1976b8c-04dc-4b67-9811-540615d1d1ba-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (chat_prompt_template | llm)\n",
    "\n",
    "chain.invoke({\"country\" : \"France\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T15:35:21.200316Z",
     "start_time": "2025-12-24T15:35:20.950700Z"
    }
   },
   "id": "d109f1afb6a29304"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
